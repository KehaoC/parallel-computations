# MPI并行矩阵乘法实验报告

**姓名：** 蔡可豪  
**学号：** 22336018

## 1. 实验目的
- 使用MPI点对点通信实现并行矩阵乘法
- 分析不同进程数量和矩阵规模对计算性能的影响
- 探讨大规模矩阵乘法和稀疏矩阵乘法的优化方向

## 2. 实验环境
- 操作系统：macOS Darwin 24.3.0
- MPI实现：OpenMPI
- 编程语言：C语言
- 编译器：mpicc (使用-O3优化)

## 3. 算法实现

### 3.1 基本思路
1. 数据分布：
   - 将矩阵A按行分块分配给各个进程
   - 将矩阵B广播给所有进程
   - 每个进程计算结果矩阵C的对应部分

2. 通信模式：
   ```c
   // 分发矩阵A
   MPI_Scatter(A, rows_per_process * matrix_size, MPI_DOUBLE,
               local_A, rows_per_process * matrix_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   
   // 广播矩阵B
   MPI_Bcast(B, matrix_size * matrix_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   
   // 收集结果
   MPI_Gather(local_C, rows_per_process * matrix_size, MPI_DOUBLE,
               C, rows_per_process * matrix_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
   ```

3. 计算过程：
   ```c
   for (int i = 0; i < rows_per_process; i++) {
       for (int j = 0; j < matrix_size; j++) {
           double sum = 0.0;
           for (int k = 0; k < matrix_size; k++) {
               sum += local_A[i * matrix_size + k] * local_B[k * matrix_size + j];
           }
           local_C[i * matrix_size + j] = sum;
       }
   }
   ```

### 3.2 关键优化
1. 内存优化：
   - 每个进程只存储必要的矩阵部分
   - 使用连续内存布局提高缓存效率
   - 及时释放不再使用的内存

2. 通信优化：
   - 使用集合通信操作减少通信开销
   - 计算过程中避免额外通信
   - 采用行式分块减少数据传输量

## 4. 实验结果与分析

### 4.1 性能数据
执行时间表（单位：秒）：
```
Process Count | 128 | 256 | 512 | 1024 | 2048
-------------|-----|-----|-----|------|------
           1 | 0.002 | 0.016 | 0.146 | 1.246 | 29.451
           2 | 0.003 | 0.013 | 0.078 | 0.959 | 16.780
           4 | 0.003 | 0.013 | 0.050 | 0.930 | 17.467
           8 | 0.001 | 0.014 | 0.053 | 0.959 | 18.890
```

### 4.2 性能分析
1. 进程数量影响：
   - 随着进程数量增加，计算时间总体呈下降趋势
   - 但在小规模矩阵时，通信开销可能抵消并行带来的收益
   - 最佳进程数量与问题规模和硬件资源相关
   - 从数据可以看出，对于2048x2048的矩阵，使用2个进程时性能最好，达到16.780秒
   - 当进程数增加到8个时，由于通信开销增加，性能反而下降

2. 矩阵规模影响：
   - 矩阵规模增大时，计算时间呈现超线性增长
   - 大规模时并行效果更明显，如2048x2048矩阵从单进程的29.451秒优化到2进程的16.780秒
   - 内存访问模式对性能影响显著
   - 对于小规模矩阵（128x128），并行反而增加了开销，单进程性能最好

## 5. 优化建议

### 5.1 大规模矩阵乘法优化
1. 分块计算策略：
   - 实现矩阵分块算法
   - 每次只加载和处理部分数据
   - 利用磁盘存储中间结果

2. 内存管理：
   - 实现流式处理算法
   - 使用内存映射文件
   - 优化数据局部性

### 5.2 稀疏矩阵优化
1. 数据结构优化：
   - 使用压缩行存储(CSR)格式
   - 只存储非零元素
   - 设计高效的索引结构

2. 算法优化：
   - 跳过零元素计算
   - 实现动态负载均衡
   - 优化稀疏矩阵专用算法

## 6. 结论与展望
1. 当前实现在中等规模矩阵上表现良好
2. 并行效率随矩阵规模增大而提高
3. 未来可进一步优化：
   - 实现混合并行（MPI + OpenMP）
   - 添加自动调优机制
   - 支持异构计算

## 7. 参考文献
1. MPI: The Complete Reference
2. Parallel Programming with MPI
3. Matrix Computations (4th Edition)